{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introdução a Bancos de Dados Vetorizados e Similaridade entre Embeddings\n",
        "\n",
        "Notebook introdutório para apresentações sobre representações vetoriais, métricas de proximidade e bancos vetorizados (com foco em Postgres + pgvector).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objetivos\n",
        "- Explicar o que são embeddings e por que precisamos deles.\n",
        "- Mostrar visualmente vetores como pontos em um espaço.\n",
        "- Comparar dot product, cosine similarity e distância euclidiana.\n",
        "- Conectar os conceitos com bancos vetorizados (pgvector, IVFFlat, HNSW).\n",
        "- Preparar os próximos notebooks com demonstrações aprofundadas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## O que são embeddings?\n",
        "Embeddings são vetores de números reais que codificam o significado de textos, imagens, usuários ou itens. Eles posicionam cada objeto em um espaço contínuo de forma que proximidade espacial corresponda a proximidade semântica.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Por que são importantes?\n",
        "- **Busca semântica:** encontra documentos relacionados mesmo sem termos idênticos.\n",
        "- **Recomendação:** ranqueia itens parecidos com o perfil do usuário.\n",
        "- **Deduplicação e segurança:** detecta versões similares de um conteúdo.\n",
        "- **Assistentes conversacionais:** mantém memória e contexto em múltiplos turnos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Onde aparecem na prática?\n",
        "- Marketplace: destacar produtos complementares.\n",
        "- Atendimento: sugerir respostas em bases de conhecimento.\n",
        "- Finanças: monitorar notícias semelhantes que impactam um ativo.\n",
        "- RH Tech: mensurar afinidade entre vagas e candidatos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intuição geométrica\n",
        "- Vetores são pontos em um espaço n-dimensional.\n",
        "- Similaridade corresponde à proximidade desses pontos.\n",
        "- Em 2D/3D conseguimos visualizar ângulos e distâncias para criar uma explicação intuitiva que vale para dimensões maiores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dependências leves\n",
        "Use apenas bibliotecas padrão. Caso esteja em um ambiente novo, execute a célula abaixo (opcional):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install --quiet numpy pandas matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(7)\n",
        "plt.style.use(\"seaborn-v0_8\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Métricas de similaridade\n",
        "As três métricas mais populares em aplicações com embeddings:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **Dot Product (Produto Interno):** valor alto indica vetores apontando para a mesma direção e magnitude elevada.\n",
        "2. **Cosine Similarity:** normaliza o comprimento e mede apenas o ângulo.\n",
        "3. **Euclidean Distance:** distância direta entre dois pontos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fórmulas amigáveis\n",
        "Para vetores `a` e `b`:\n",
        "- `dot(a, b) = Σ (a_i * b_i)`\n",
        "- `cos(a, b) = dot(a, b) / (||a|| * ||b||)`\n",
        "- `d(a, b) = sqrt(Σ (a_i - b_i)^2)`\n",
        "\n",
        "> Dot/Cos retornam \"quanto maior, mais parecido\". Distância retorna \"quanto menor, mais próximo\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demonstração prática\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_POINTS = 5\n",
        "vectors = np.random.randn(NUM_POINTS, 2)\n",
        "labels = [f\"v{i+1}\" for i in range(NUM_POINTS)]\n",
        "\n",
        "df_vectors = pd.DataFrame(vectors, columns=[\"x\", \"y\"])\n",
        "df_vectors[\"label\"] = labels\n",
        "df_vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dot_product(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.dot(a, b))\n",
        "\n",
        "\n",
        "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    denom = np.linalg.norm(a) * np.linalg.norm(b)\n",
        "    if denom == 0:\n",
        "        return 0.0\n",
        "    return float(np.dot(a, b) / denom)\n",
        "\n",
        "\n",
        "def euclidean_distance(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.linalg.norm(a - b))\n",
        "\n",
        "\n",
        "anchor = vectors[0]\n",
        "comparisons = vectors[1:]\n",
        "\n",
        "rows = []\n",
        "for label, vec in zip(labels[1:], comparisons):\n",
        "    rows.append(\n",
        "        {\n",
        "            \"par\": f\"{labels[0]} vs {label}\",\n",
        "            \"dot\": dot_product(anchor, vec),\n",
        "            \"cosine\": cosine_similarity(anchor, vec),\n",
        "            \"euclidean\": euclidean_distance(anchor, vec),\n",
        "        }\n",
        "    )\n",
        "\n",
        "metrics_df = pd.DataFrame(rows)\n",
        "metrics_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "colors = [\"#E15759\"] + [\"#4E79A7\"] * (NUM_POINTS - 1)\n",
        "ax.scatter(df_vectors[\"x\"], df_vectors[\"y\"], c=colors, s=120)\n",
        "\n",
        "for _, row in df_vectors.iterrows():\n",
        "    ax.annotate(row[\"label\"], (row[\"x\"] + 0.05, row[\"y\"] + 0.05))\n",
        "\n",
        "ax.axhline(0, color=\"gray\", linewidth=0.6)\n",
        "ax.axvline(0, color=\"gray\", linewidth=0.6)\n",
        "ax.set_title(\"Vetores 2D de exemplo\")\n",
        "ax.set_xlabel(\"Dimensão 1\")\n",
        "ax.set_ylabel(\"Dimensão 2\")\n",
        "ax.set_xlim(df_vectors[\"x\"].min() - 0.6, df_vectors[\"x\"].max() + 0.6)\n",
        "ax.set_ylim(df_vectors[\"y\"].min() - 0.6, df_vectors[\"y\"].max() + 0.6)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretação rápida\n",
        "- **Dot Product:** considera magnitude + alinhamento; ideal quando embeddings carregam pesos ou probabilidades.\n",
        "- **Cosine Similarity:** ignora magnitude e foca no ângulo; ótima para textos de tamanhos diferentes.\n",
        "- **Euclidean Distance:** distância direta; intuitiva quando vetores já estão normalizados ou descrevem coordenadas físicas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bancos vetorizados em alto nível\n",
        "Armazenar milhões de embeddings exige estruturas otimizadas para buscar vizinhos rapidamente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Postgres + pgvector\n",
        "- Adiciona o tipo `vector` ao Postgres, permitindo armazenar embeddings lado a lado com dados relacionais.\n",
        "- Suporta consultas com `dot_product`, `cosine_distance` e `euclidean_distance` diretamente em SQL.\n",
        "- Índices ANN disponíveis:\n",
        "  - **IVFFlat:** organiza vetores em listas invertidas; ótimo para bases grandes, requer `ANALYZE` após cargas.\n",
        "  - **HNSW:** grafo hierárquico navegável, excelente precisão e latência baixa com custo de memória maior.\n",
        "- Escolha a métrica ao criar o índice (`vector_ip_ops`, `vector_cosine_ops`, `vector_l2_ops`) para alinhar com o comportamento do modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quando usar cada métrica?\n",
        "| Cenário | Métrica | Motivação |\n",
        "| --- | --- | --- |\n",
        "| Ranking com pesos embutidos (popularidade, confiança) | Dot Product | Mantém magnitude absoluta dos vetores.\n",
        "| Textos de tamanhos distintos / pura semântica | Cosine Similarity | Normaliza comprimento e mede apenas o ângulo.\n",
        "| Dados normalizados ou coordenadas físicas | Euclidean Distance | Distância direta, intuitiva para clustering/limites.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusão\n",
        "- Embeddings traduzem contexto em números comparáveis.\n",
        "- Dot Product, Cosine e Euclidean oferecem perspectivas complementares.\n",
        "- pgvector leva essas métricas para dentro do Postgres com índices ANN (IVFFlat/HNSW).\n",
        "- **Próximos notebooks:** veremos casos completos para cada métrica (recomendação, busca semântica e clustering).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
